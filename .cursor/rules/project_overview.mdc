---
description: 
globs: 
alwaysApply: true
---
## Using baml file
Review BAML.md as needed for details on how to work with baml files.

## Build & Test Commands
- Install dependencies: `pip install -e ".[dev]"`
- Generate BAML client: `baml generate`
- Run all tests: `pytest`
- Run single test: `pytest tests/path_to_test.py::test_function_name -v`
- Run tests with coverage: `pytest --cov=baml_client`
- Lint code: `ruff check .`
- Type check: `mypy .`

## Code Style Guidelines
- **Formatting**: Use Black for Python code formatting
- **Imports**: Group imports in this order: standard library, third-party, local application
- **Types**: Use type annotations for all functions and methods
- **Naming**: 
  - Classes: PascalCase
  - Functions/methods: snake_case
  - Constants: UPPER_SNAKE_CASE
  - BAML entities: PascalCase (functions, clients, classes)
- **Error Handling**: Use specific exceptions with helpful error messages
- **BAML Patterns**: Keep prompt templates clean and well-structured see BAML.md for details.
- **Tests**: Write test cases for all BAML functions to ensure prompt behavior

## Core Functionality and Purpose

The Contractor App is a web-based tool that helps contractors (e.g. general contractors, landscapers) generate detailed project estimates using AI. Its core functionality is to take various inputs about a project – such as written descriptions, notes, images (photos or plans), and even audio/video walkthroughs – and produce a structured cost estimate with line items, cost ranges, timelines, and more. By aggregating all project information and leveraging a large language model (LLM), the app saves time in drafting estimates and ensures important details aren’t overlooked. The app is specifically designed for use in fields like home renovation or landscaping, where contractors can upload project details and receive an **AI-generated estimate** as a starting point for proposals.

Key features include:

* **Project Data Input:** Users (contractors) create a project and input relevant data. This can be a written project description, client requirements, budgets, etc., and they can upload supporting files (e.g. sketches, photos of the site, measurement notes, or audio notes from a walkthrough). Each file can be described or categorized for context. All these files are later fed into the AI for analysis.

* **AI-Powered Estimate Generation:** With one click (“Generate Estimate”), the app sends the project’s files and info to an AI assistant specialized in construction estimation. The AI analyzes the scope and details from the inputs and returns a **structured JSON** result describing the project estimate. This structured estimate includes a summary, total cost range, estimated timeline, key assumptions, and an itemized list of line items (each with description, category, quantity, unit, cost range, etc.). The estimate also notes any missing information and potential risks the AI identified, giving the contractor insight into what else might be needed for accuracy.

* **Interactive Refinement via Chat:** The app features an integrated chat interface where the contractor can converse with the AI assistant. This allows users to ask questions about the estimate (“Why is this item so expensive?”) or request changes (“Can we use a cheaper material for the flooring?”). The AI will either answer the question or, if the request involves altering the plan, it can trigger an **estimate update** – regenerating the estimate with the requested changes applied. The chat history and current estimate are taken into account so the AI has full context of the project. This conversational loop lets contractors iteratively refine the estimate in a natural way, as if consulting an expert assistant.

* **Project Management and Persistence:** Each project’s data (files, generated estimate, chat history, etc.) is saved. Contractors can return to a project later, review the latest estimate, continue the chat, or upload new information. The app uses Supabase (a backend-as-a-service with Postgres) to persist projects and user data, enabling a contractor to manage multiple projects in one place. The goal is to contextualize the AI’s output for real-world use: the estimate can be further edited or exported as needed when preparing a formal bid for a client.

Overall, the Contractor App’s purpose is to streamline the early phases of project planning for contractors and landscapers by providing quick, AI-generated estimates and a responsive assistant to answer questions or adjust the plans. This reduces the manual effort of drafting estimates and allows contractors to explore “what-if” scenarios with clients (e.g. different materials or project scopes) on the fly. It’s an assistive tool, meant to augment the contractor’s expertise with speedy document analysis and estimate computation.

## Web UI Architecture and Operation

**Tech Stack & Organization:** The front-end is built with **Next.js 13** (React framework). This project is a monorepo that includes both the Next.js app (the web UI) and a Python service for AI (called “LangGraph”) in one repository. The Next.js app resides in `apps/web/` and uses the App Router (with React Server Components and Server Actions) for a modern, dynamic user experience. Styling and component structure follow typical React patterns, with an emphasis on clarity for the estimate and chat interfaces.

**Layout and Navigation:** The app uses a main layout component (often referred to in code as `AppClientShell`) that defines the overall page structure. This includes a top navigation bar (with links like “Projects” and account settings/logout) and a left sidebar for project-specific navigation. When a project is selected, the sidebar shows options to switch between the **Estimate view** and the **Files view** for that project, and also provides access to the chat feature. The `AppClientShell` also provides a React Context (`ViewContext`) to share state between components – for example, it keeps track of which project is currently open and whether the Estimate or Files view is active. It also holds a callback function that the chat interface can call when an AI-driven estimate update is triggered, allowing different parts of the UI to stay in sync.

**Project Dashboard:** Upon logging in, users see a dashboard listing their projects (each with a name/description). They can select an existing project or create a new one. (In the seed data, for example, two sample projects are listed: “Bathroom Renovation” and “Kitchen Refresh”.) The dashboard is implemented in Next.js’s `app/dashboard/page.tsx` and likely uses Supabase queries to fetch the list of projects for the logged-in user. Selecting a project navigates to that project’s page (URL like `/projects/[id]`).

**Project Page (Estimate and Files views):** The project page is the core workspace. It has two main sub-views which the user can toggle:

* **Estimate View:** This view displays the AI-generated estimate for the project, or a placeholder if no estimate has been generated yet. On initial load, the Next.js server component fetches the project details and the current saved `ai_estimate` JSON from the database. If an estimate exists, the page renders the structured data: typically a project overview (description, total estimated cost range, timeline, confidence) and the list of detailed line items (grouped by category perhaps). If no estimate is present, the UI indicates that and prompts the user to generate one. There is a button (e.g. “Generate Estimate”) to initiate the AI process. The Estimate view is interactive: if the user starts a generation, the UI state `estimateStatus` switches to "processing" and a loading indicator or spinner is shown in place of the estimate. The code sets this state and begins polling for the result. If the result comes back successfully, `estimateStatus` becomes "completed" and the new estimate data is displayed; if it fails, the state becomes "failed" and an error message is shown. These states ensure the user gets feedback (loading vs. error vs. showing the data). The Estimate view is primarily read-only (apart from triggering generation) – any fine adjustments are done either via chat or by re-running with changes.

* **Files View:** This view lists all files uploaded for the project and allows adding new files or notes. It’s essentially a file manager for project inputs. The page fetches the list of files from the `files` table in Supabase when loading. Each file entry shows its name and a description (for example, “current\_bathroom.png – Current bathroom photo”). Users can upload additional files (images, PDFs, text notes, audio clips, etc.) via an upload form. The upload process is handled by a Next.js server action `uploadFile` which writes the file to Supabase Storage and creates a record in the `files` table. There is typically an 8MB size limit enforced for uploads (to avoid very large files). After uploading, the file list refreshes. These files will be used as input for the next estimate generation. The Files view lets contractors gather all relevant project info in one place.

The **layout** keeps the Estimate view and Files view separate for clarity – the contractor can switch between reviewing the current estimate and managing the input files. The left sidebar links (“Estimate” and “Files”) control which portion is visible at a time (this was part of a planned UI enhancement to make navigation clearer).

**Chat Panel (AI Assistant Chat):** A standout feature of the UI is the chat panel, which is typically shown on the right side or as an overlay when activated. The chat panel can be opened (for example via a “Chat” button or when clicking “New Chat”) and is always contextual to the current project. In the code, `ChatPanel` is a component that receives the current `projectId` and manages a chat conversation thread. Users can have multiple chat threads per project (e.g., one might be “Initial Questions”, another “Follow-up changes”), and a “New Chat” button will start a fresh thread (with its own context). The chat UI displays the conversation as a series of messages or events.

When the user types a message and sends it, the chat panel does the following: it immediately renders the user’s message in the conversation (optimistic UI update), then calls a server action to actually post the message to the backend. If it’s the first message in a new thread, it calls an action to create a new chat thread and post the message together; otherwise it posts to the existing thread. These server actions (`createChatThreadAndPostMessage` and `postChatMessage`) will save the message into the database (Supabase `chat_events` table) and then invoke the AI to get a reply. While waiting for a reply, the UI might show a loading indicator for the assistant’s response.

The **chat backend flow** is described in detail later, but from the UI perspective, the chat panel polls for new events every few seconds so that the assistant’s reply or any system events (like “Estimate updated”) appear promptly. Once the assistant’s reply comes in, the chat panel adds it to the conversation. If the AI decides it needs to update the estimate, the UI will get a special event indicating an update is in progress. In the chat, this appears as a system message like “🔄 Agent is updating the estimate...” (often labeled as an `UpdateEstimateRequest` event) which can be shown in a collapsed form with details of what changes are being made. The chat panel is designed to detect this and visually distinguish system events from normal messages. It also uses the context callback from the `AppClientShell`: when an estimate update is triggered via chat, the chat component calls that callback, which flips the Project page’s state to the loading mode (so the estimate view shows a spinner). In essence, the chat panel and the main project page communicate so that if the AI is going to regenerate the estimate, both the chat and the estimate UI reflect that state change. After the update is done, another event (`UpdateEstimateResponse`) is added to the chat (e.g. “✅ Estimate updated successfully.”), and the new estimate data is loaded into the Estimate view automatically.

**Server Actions and API calls:** Next.js server actions (in `actions.ts`) form the bridge between the UI and both the database and the LangGraph AI service. Notable server actions include:

* `uploadFile` – as mentioned, saves an uploaded file to Supabase Storage and inserts a record in `files`.

* `startEstimateGeneration` – kicks off the process to generate an AI estimate for a project. This function gathers all files for the project from Supabase, generates signed URLs for each (so they can be accessed by the LangGraph service), and then calls the **LangGraph API** to initiate an AI run. It does this by first creating a new “thread” and then starting a “run” with the appropriate assistant (the assistant is essentially the AI program that knows how to process the files). The call is made via `fetch()` to the LangGraph service’s endpoints (URL from an environment variable) with an API key for authentication. When a run is started successfully, this action records a new entry in the `task_jobs` table with status “processing”. It then returns control to the UI (so `startEstimateGeneration` doesn’t wait for completion; it triggers the background job and returns immediately).

* `checkEstimateStatus` – allows the UI to poll the status of a running job. This action likely takes a `projectId` (or job ID), queries the `task_jobs` table for that project’s latest job status, and if needed, may also query the LangGraph service for more detailed status. If the job finished, `checkEstimateStatus` will fetch the results. In the app’s flow, when a job finishes successfully, `checkEstimateStatus` calls another helper (sometimes referred to as `processCompletedRun`) which retrieves the output (the JSON estimate) from LangGraph, updates the project’s record in the `projects` table with the new `ai_estimate`, and marks the job status as “completed”. If the job failed, the status is marked “failed”. The UI’s polling uses this action to know when to stop showing a loading spinner and instead show results or an error.

* Chat-related actions: `createChatThreadAndPostMessage` and `postChatMessage` handle chat input. They insert a new row in `chat_threads` (if needed) and a new `chat_events` entry for the user message. They then compile the conversation history and current estimate from the database and call the LangGraph’s chat decision function to get a reply (described later). They finally insert the assistant’s reply as another `chat_events` entry. These actions return data back to the UI indicating what happened (including whether an estimate update was triggered). The chat UI will use that to update state (e.g., if `updateTriggered: true` comes back, it knows an `UpdateEstimateRequest` event was added and the estimate regeneration started).

In summary, the web UI is a Next.js application that provides: a **project dashboard**, a detailed **project page** with two main views (Estimate and Files), and an integrated **chat interface**. The UI relies heavily on server actions to handle back-end operations like database reads/writes and calling the AI service. Supabase is used for storing persistent data (projects, files, chats), while the heavy AI work is offloaded to the LangGraph service via API calls. The design ensures a smooth user experience: file uploads are straightforward, generating an estimate provides feedback while processing, and the chat enables a conversational refinement of that estimate. All these components work together to fulfill the app’s purpose of making project estimation faster and easier for the user.

## Supabase Integration (Data Storage and Auth)

Supabase serves as the primary backend for data persistence and user management in the Contractor App. It provides a Postgres database, file storage buckets, and authentication services. Here are the key aspects of how the app uses Supabase:

* **User Authentication:** Supabase’s Auth is used to handle user accounts and login. When new contractors sign up or log in, their credentials are managed by Supabase (with email/password, etc.). The app’s Next.js pages include routes for login and email confirmation (for signup). Once logged in, the user’s session allows the front-end to interact with Supabase (through the provided API/service key) for that user’s data. Each project and resource is linked to a `user_id` so that users only access their own projects.

* **Database Schema:** The app’s relational data is stored in a set of tables. The main tables and their roles are:

  | **Table**      | **Purpose**                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |
  | -------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
  | `projects`     | Stores each project’s info. Key fields include a unique `id`, a `user_id` (owner), a project `name`, a textual `description` (brief summary), a longer `project_info` (possibly detailed notes or initial input text), and an `ai_estimate` JSON field that holds the latest AI-generated estimate for the project. When an estimate is generated, this JSON is saved here so it can be displayed later without re-running the AI.                                                                                                                                                                                                                                                                                                                                                                                                    |
  | `files`        | Stores metadata for files uploaded to each project. Fields include an `id`, `project_id` (which project the file belongs to), `file_name` (the original filename), a `description` (user-provided context for the file), and a `file_url`. The `file_url` is the path in the Supabase Storage bucket where the file content is stored (for example, `project1/current_bathroom.png`). The actual binary content lives in the storage, not in this table, but this record links it to the project.                                                                                                                                                                                                                                                                                                                                     |
  | `chat_threads` | Represents a chat session (conversation) associated with a project. Fields include an `id` (UUID), `project_id` (which project the thread is for), a `name` (a user-friendly title for the chat, e.g. "Initial Bathroom Ideas"), and timestamps. Multiple chat threads per project are allowed. This lets a contractor have separate thematic conversations if needed.                                                                                                                                                                                                                                                                                                                                                                                                                                                                |
  | `chat_events`  | Stores the individual messages/events in each chat thread. Each row has an `id`, `thread_id` (linking to a chat thread), an `event_type` (textual type identifier), a JSON `data` field, and a timestamp. The `event_type` can be `"UserInput"`, `"AssisantMessage"` (note the spelling in code, but effectively assistant message), `"UpdateEstimateRequest"`, or `"UpdateEstimateResponse"` – these correspond to the types of chat events defined in the system. The `data` JSON contains the content of the message or event (e.g. for UserInput/AssistantMessage it has a `"message"` text; for UpdateEstimateRequest it has `"changes_to_make"` text; for UpdateEstimateResponse it might have a success flag or error message). This table essentially logs the full conversation history and any system events for each chat. |
  | `task_jobs`    | Tracks asynchronous AI jobs (particularly, the estimate generation runs). Fields include an `id`, `project_id`, `thread_id` (if the job was triggered from a chat thread), a `run_id` and `status`, plus `job_type`. When the user starts an estimate generation, a new task\_job is inserted with status `"processing"`. It gets updated to `"completed"` or `"failed"` when the AI service finishes. This table helps the app know if an estimate is currently being generated and to retrieve the results when ready.                                                                                                                                                                                                                                                                                                              |

  These tables allow the app to reconstruct state at any time. For example, when loading a project page, the app selects from `projects` (to get the project info and maybe existing estimate JSON) and from `files` (to list files). If the user opens a chat, the app pulls the thread from `chat_threads` and the last N events from `chat_events` to show the history. Because all chat and estimate updates are stored, the user can leave and come back later to see the conversation and the latest estimate.

* **File Storage:** Supabase includes an S3-like object storage. The app uses this to store the actual content of files (images, PDFs, audio, etc.) that the user uploads. The `file_url` field in the `files` table is essentially the path in the storage bucket. In the development setup, a bucket (defined by `SUPABASE_STORAGE_BUCKET` env var) is used (e.g., “contractor-app-dev”). When a file is uploaded via the app, the `uploadFile` action takes the file binary and calls Supabase Storage’s API to upload it at a path like `{projectId}/{fileName}`. Supabase returns an error if upload fails, which the app handles (e.g., file too large or network issue). On success, the file is stored and accessible via the storage service.

  Importantly, when the AI service (LangGraph) needs to read these files, the app does **not** send the raw file content through the Next.js server due to size and efficiency concerns. Instead, it generates a **signed URL** for each file. A signed URL is a time-limited public link that allows secure download of the file without requiring a separate login. The `startEstimateGeneration` action uses the Supabase server client to create signed URLs for each file to be processed. Each URL is valid for a short duration (in dev, 1 hour) and is passed to the LangGraph service. The Python service then uses these URLs to download the file contents directly. This approach is efficient: it offloads file transfer to cloud storage (which is optimized for serving files) and avoids running large file data through the Next.js server process. Once the AI has processed the file, the URL expires, keeping things secure. If a signed URL generation fails for some reason (perhaps the file was not found or permissions issue), the app will report an error and abort the generation.

* **Data Access in Next.js:** The app uses Supabase both on the client (for some real-time or lightweight queries) and on the server (in server actions). A helper `createClient` function is used in server actions to instantiate a Supabase client with service-role access (using a service key). This allows the server code to perform admin-level operations (like inserting rows, selecting without row-level security restrictions, etc.). For example, `startEstimateGeneration` uses this to fetch all files for a project and to insert a task\_jobs entry. On the client side, there might be a limited Supabase client (initialized with the user’s JWT) for any direct client needs, but most data fetching is done via the Next.js SSR/ISR or server actions for simplicity and security.

* **Row Security & Policies:** (Not deeply covered in the code excerpts, but presumably) Supabase ensures that users can only access their own projects/files. The Next.js server actions themselves filter by `user_id` (for projects) or join through a user’s project. For instance, when fetching files, the code provides a `projectId` which inherently is tied to a user’s project; an additional check could be in place to verify the project belongs to the session user (not shown in snippet, but a best practice). Supabase Row Level Security (RLS) might be configured so that selecting from `projects` or `files` returns only rows for the authenticated user. The seed data indicates test users and projects are linked via user\_id.

* **Example Data Flow:** If user "[alice@example.com](mdc:mailto:alice@example.com)" logs in and creates a project, a new row in `projects` is created with `user_id = alice`. When Alice uploads `floorplan.png` to that project, a row in `files` is added with that project\_id, and the file is stored. When Alice clicks "Generate Estimate", the app fetches all `files` where `project_id = that project` (ensuring they are her files), creates signed URLs for each, and calls LangGraph. Once done, it updates that project’s `ai_estimate`. If Alice opens a chat, a new `chat_threads` row links to her project, and any messages she sends create `chat_events` tied to that thread. All queries ensure that the `user_id` matches Alice’s session so data isn’t leaked across accounts.

In summary, **Supabase is the glue for data**: it stores project info, files metadata, chat records, and job statuses. It also authenticates users. The Next.js app relies on Supabase for persistent state so that the AI service (which is stateless in terms of data) can be used asynchronously and results stored. New engineers should be aware of the Supabase configuration (check the `.env` for the project, which should have the Supabase keys and the bucket name) and the table schemas. Most interactions with Supabase are encapsulated in the server action functions, making it straightforward to locate where reads/writes occur.

## LangGraph Service and LLM Integration

The heavy lifting for AI-driven features is handled by the **LangGraph service**, a Python application that runs separately from the Next.js web app. LangGraph is responsible for interfacing with language models (like OpenAI GPT) and processing the inputs into the desired outputs (estimates, answers, etc.). It’s designed so that the potentially slow or large computation (LLM calls, file downloading, etc.) happens off the main web thread. Here’s an overview of LangGraph and how it works:

**Architecture & Communication:** LangGraph runs as a back-end service (likely a FastAPI or similar web server) exposing a simple HTTP API that the Next.js app calls. From the Next side, we saw calls to endpoints like `/threads` and `/threads/[id]/runs` with an API key. This suggests LangGraph has an API where you can create a new “thread” (context container) and then start a “run” (execution of an AI workflow) within that thread. The API key ensures only authorized calls (like from our app) can trigger it. The separation means you can scale or deploy the AI service independently (possibly on a server with more memory or special environment) and keep the web front-end responsive.

**BAML – Defining AI Logic:** A core concept in this project is the use of **BAML (Boundary Markup Language)** for AI interactions. BAML is a domain-specific language for specifying LLM prompts, input/output schemas, and tying them into client code. Instead of hardcoding prompts in Python or JavaScript, the developer writes BAML files (found in the `baml_src/` directory) describing the data structures and prompts. The BAML CLI then **generates two client libraries** from these definitions: one in TypeScript (`apps/web/baml_client/`) and one in Python (`apps/langgraph/baml_client_py/`). This ensures the front-end and back-end have synchronized interfaces for calling AI functions. For example, if a BAML file defines a function `GenerateProjectEstimate` with certain input/output types, the TS client and Python client will both have a `b.GenerateProjectEstimate()` method available to call, abstracting the actual API call.

Key BAML-defined components for this app include:

* **Data Schemas**: The BAML files define classes such as `EstimateLineItem`, `ConstructionProjectData`, `InputFile`, etc., which mirror the data we care about. For instance, `ConstructionProjectData` has fields like project\_description, estimated\_total\_min/max, estimated\_timeline\_days, confidence\_level, a list of `estimate_items` (each an `EstimateLineItem`), `next_steps`, `missing_information`, `key_risks`, etc.. These act as a **schema** for the JSON output of the estimate. The LLM is instructed to output data conforming to this schema (so the app can easily parse it). Similarly, `InputFile` includes fields for a file’s name, type (MIME type), optional content or image/audio data, etc., and classes for `VideoFrame` and `ProcessedVideo` exist to structure video analysis results.

* **Clients (AI model endpoints)**: BAML allows abstracting different model endpoints as “clients.” In the BAML files, we see references to `client OpenaiFallback` and `client GeminiProcessor`. These likely correspond to configurations where:

  * **OpenaiFallback**: uses OpenAI’s GPT models (perhaps tries GPT-4 and falls back to GPT-3.5 if needed, or uses GPT-3.5-turbo by default with an option to upgrade). This client is used for the main text-heavy tasks like generating the estimate and answering chat questions.
  * **GeminiProcessor**: (the name hints at Google’s "Gemini", a multimodal model) is used for tasks involving audio/video. In our context, `ProcessAudio` and `ProcessVideo` functions are assigned to the `GeminiProcessor` client. This could mean either an actual call to a different API that can handle audio/video, or simply a placeholder name if such a model is not yet available. The code suggests the team planned for advanced multimodal processing (Gemini is a next-gen model that can handle text, images, etc.), even if currently the implementation might be basic or stubbed.

* **Functions (Prompt Templates)**: The BAML file `file_processor.baml` defines the function `GenerateProjectEstimate(files: InputFile[], existing_estimate: ConstructionProjectData?, requested_changes: string?) -> ConstructionProjectData`. This is the heart of the estimation logic. The prompt template for this function is written in the BAML file, interleaving the provided inputs into a textual prompt for the LLM. In simplified terms, the prompt says: "You are an AI assistant specialized in construction project analysis. Using solely the provided files and information (which are listed below in <file> tags) and considering any existing estimate and requested changes, produce a detailed cost estimate in JSON format." It literally injects each file’s content or image data into the prompt structure and, if an existing estimate is given (like when doing an update), it includes that as well, and likewise includes a description of requested changes if provided. Finally, it instructs the model: “Based solely on the provided information, generate a detailed estimate... Output the estimate as a JSON object conforming to the specified schema.”. This ensures the model’s response will directly be a JSON matching our `ConstructionProjectData` schema (or an error if it couldn’t, but ideally it does).

  Another function in BAML is `DetermineNextStep(thread: BamlChatThread, current_estimate: ConstructionProjectData) -> Event` for the chat logic. Its prompt tells the AI to behave like a helpful construction estimator assistant, consider the conversation history and current estimate, and decide on the next step. The conversation history is fed in as a series of events (user or assistant messages, etc.) and the model must output one new Event – either a normal assistant message or an instruction to update the estimate. This is how the AI in chat can say, for example, “I will update the estimate with those changes” by returning an `UpdateEstimateRequest` event with details of changes needed.

  And as mentioned, there are `ProcessAudio(audio: InputFile) -> string` and `ProcessVideo(video: InputFile) -> ProcessedVideo` functions defined to handle audio and video files respectively. Their prompts are simpler (e.g., for audio: “You’re given an audio file, transcribe it.” for video: “You’re given a video, provide a summary.”) – these would use the GeminiProcessor which presumably might do speech-to-text or video analysis.

**LangGraph Workflow:** The Python side uses the definitions above to actually execute these functions. The code in `apps/langgraph/src/file_processor/graph.py` sets up a workflow graph for the estimate generation process. Specifically, it creates a `StateGraph` with two main nodes: `process_files` and `generate_estimate`, in that order. This means when a run is executed for the `file_processor` assistant, it will first run `process_files`, then feed its output into `generate_estimate`.

* **File Processing (process\_files):** This step takes the input state (which includes the list of files and possibly existing estimate/changes) and prepares the files for the LLM. The code loops through each file in `state.files` and handles it according to its MIME type:

  * If the file is an image (`file.type` starts with `"image/"`), the service downloads it from the provided URL (using an async HTTP client) and converts it to a base64-encoded string. It then wraps that in an `Image` object (provided by the BAML Python client library) and assigns it to `file.image_data`. This way, the image data can be included in the prompt. (The BoundaryML/BAML framework likely handles embedding images in the prompt or converting them to descriptions via the model – possibly the model knows how to handle the `image_data` type.)
  * If the file is audio (`"audio/"`), it similarly downloads the bytes and wraps them in an `Audio` object. Then, it calls the BAML function `ProcessAudio` on that audio object to get a transcription. The result (transcribed text) is stored as the `file.content`. Essentially, the service itself uses the AI to transcribe audio notes into text before generating the estimate. After transcription, it clears the audio\_data to save memory and marks the file’s type as text (since now we have text content).
  * If the file is text (`"text/"` type, e.g. a .txt or .md note), it just downloads the text content and puts it into `file.content`.
  * If the file is a video (`"video/"`), the current implementation downloads the video bytes and stores them as a base64 string in `file.content` (for lack of a better place). It doesn’t actually process the video further in this step, but it leaves a note in code that a dedicated video processing might happen later. (Likely, they intended to call `ProcessVideo` here similar to audio, but perhaps they didn’t have a working multimodal model yet. The video bytes could potentially be sent to an API or model for analysis in the future.)
  * If a file’s type is unrecognized, it defaults to trying to download as text and store that.

  The output of `process_files` is a list of `InputFile` objects with their `content` or `image_data` or `audio_data` fields populated appropriately. These get passed on to the next step.

* **Estimate Generation (generate\_estimate):** This step calls the BAML function `GenerateProjectEstimate` using the processed files and any existing estimate/changes from the state. In code, it looks like:

  ```python
  response: ConstructionProjectData = await b.GenerateProjectEstimate(
      files=state.files,
      existing_estimate=state.ai_estimate,
      requested_changes=state.requested_changes
  )
  ```

  Here, `state.files` is the list prepared by process\_files, `state.ai_estimate` would be a ConstructionProjectData object if an existing estimate was provided (or None if this is the first run), and `state.requested_changes` is the text of requested changes if this run was triggered by a chat request. The `b.` object is the BAML Python client (imported from `.baml_client`) which knows how to call the AI with the prompt. Under the hood, this likely sends the prompt to OpenAI’s API (using the API key and model specified in the OpenaiFallback client configuration) and waits for the JSON response. If successful, it returns a `ConstructionProjectData` Pydantic model instance (already parsed from JSON). The code then simply returns that as `{"ai_estimate": response}` to indicate the result of this node.

  If an exception occurs (e.g., the AI API fails or returns invalid JSON), it logs an error. There might be retry or error handling at a higher level, but generally that would result in the job being marked failed.

* **Putting it together:** The LangGraph service registers this workflow under an assistant ID (the Next.js code refers to `assistant_id: 'file_processor'` when starting a run). So when the front-end calls `/threads/:id/runs` with `assistant_id = 'file_processor'`, LangGraph knows to execute the `estimate_graph` we just described. It runs `process_files`, then `generate_estimate`, and yields a result. Because this can take some time (especially if the LLM call is slow or there are many files to download), the LangGraph service likely handles runs asynchronously. The `run_id` that Next.js receives is an identifier to query the status or result later.

  LangGraph may store intermediate state or results in memory or on disk for the run. However, since the Next app is handling persistence in Supabase, LangGraph might simply keep things in memory until queried. The Next app uses the `thread_id`/`run_id` to fetch results or check status. Possibly, LangGraph’s API has an endpoint like GET `/threads/:id/runs/:run_id` to get the result or status, which `checkEstimateStatus` might call. In our case, the Next app chose to record the results in the database and not poll the LangGraph directly beyond starting the run – it polls its own `task_jobs` and then possibly does one fetch at completion to get the output.

* **Chat Decision Making:** In addition to the file\_processor workflow, LangGraph also would host the chat-related logic. The `DetermineNextStep` function (from `chat.baml`) would be called by the Next.js chat actions. This is likely done via a synchronous call rather than the threaded runs (since a chat response is quick and can be handled in real-time). The Next server action might directly use the TypeScript BAML client (`b.DetermineNextStep(...)`) which could call LangGraph in a blocking manner, or possibly they set up a synchronous API for it. (Alternatively, they might handle chat entirely on the Next side if they had a Node client for OpenAI. But given they have BAML, they likely route it through the same system for consistency). Each chat message from the user triggers `DetermineNextStep` with the full conversation. The model’s response (Event) is then returned. If it’s an `AssisantMessage`, it’s saved to `chat_events`; if it’s an `UpdateEstimateRequest`, the Next app knows to start an async estimate job as described above. Once that job finishes, the Next app inserts an `UpdateEstimateResponse` event into the chat. From LangGraph’s perspective, the actual work of updating the estimate is just another run of the file\_processor assistant (with `existing_estimate` and `requested_changes` filled in). There isn’t a separate AI function for "apply changes" – it uses the same `GenerateProjectEstimate` prompt but provides context about changes to make.

* **Performance considerations:** The LangGraph service is designed to allow concurrency and background tasks. By having threads and runs, multiple estimates (for different projects or even iterative updates) can be processed in parallel or sequence without blocking the user interface. The Next app’s `task_jobs` table is the source of truth for whether a particular project has a job running. The architecture ensures that even if the AI takes, say, 30 seconds to generate a complex estimate, the user can continue interacting with the app (they could navigate elsewhere, or look at files, etc.) and check back on the result.

**Summary of the End-to-End Flow (Estimate Generation):** To put all the pieces together from user action to result:

1. **User triggers estimate generation** (either by clicking "Generate Estimate" on a project with given files, or indirectly via a chat request). The UI calls `startEstimateGeneration(projectId, requested_changes?)` on the server.

2. **Next.js server action prepares data:** It fetches the list of files for the project from Supabase and for each file, generates a signed download URL. It also fetches the project to see if there's an existing `ai_estimate` (and passes that as `existing_estimate` if this is an update). It then calls the LangGraph API:

   * POST `/threads` to create a new thread (gets back a `thread_id`).
   * POST `/threads/{thread_id}/runs` with JSON body specifying `assistant_id: "file_processor"` and the input state (files with URLs, existing\_estimate, requested\_changes). This returns a `run_id` if accepted.

3. **Next records the job:** It inserts a new row in `task_jobs` with the project\_id, thread\_id, run\_id, status "processing", and notes if it was triggered via a chat thread. Then the server action returns to the UI, confirming that the job started.

4. **LangGraph processes the run:** In the background, LangGraph (upon receiving the run request) starts executing the `file_processor` workflow. It downloads each file using the signed URLs (logging any issues), transcribes audio if needed, etc.. After preparing files, it calls OpenAI with the big prompt assembled from those files and waits for the completion. If successful, it has a `ConstructionProjectData` result object.

5. **LangGraph signals completion:** Depending on implementation, LangGraph might update an internal status for that run to "succeeded" and store the result. (If the Next app were polling LangGraph, it would retrieve result via API. In our design, the Next app instead polls its own DB.)

6. **Next UI polling:** The project page enters a loop where every few seconds it calls `checkEstimateStatus(projectId)`. This server action checks the `task_jobs` table for the latest job for that project. It sees status "processing" initially. Possibly it also calls LangGraph API `/threads/{id}/runs/{run_id}` to double-check status and to handle timeouts. When LangGraph finishes, the Next action finds out (either by LangGraph callback or by noticing the job status changed, but likely it needs to ask LangGraph).

7. **Retrieving the result:** Once the run is done, `checkEstimateStatus` will call LangGraph to get the output. The LangGraph service provides the `ConstructionProjectData` JSON (perhaps via a GET request or it could have been included in the original run response if they waited – but likely they didn't wait in the initial call). Next then calls an internal helper `processCompletedRun`: it takes the JSON, updates the `projects` table’s `ai_estimate` with this JSON (as text), and sets the `task_jobs.status` to "completed". If this job was triggered by chat, it also creates an `UpdateEstimateResponse` event in `chat_events` indicating success.

8. **UI updates:** The polling action returns something like `{ status: 'completed' }` to the front-end, at which point the React state `estimateStatus` becomes "completed". The project page then fetches the updated `ai_estimate` from Supabase (or the action might return the data directly) and displays the new estimate. The loading indicator is removed. In the chat panel, if one was open and initiated this, it will now show a system message "Estimate updated" as well.

For **Chat**: The flow is slightly different in that step 2 is initiated by a different action (post message -> DetermineNextStep). If DetermineNextStep returns an `UpdateEstimateRequest`, then step 1-8 above are invoked as a subroutine (with `requested_changes` filled from the AI’s message). The chat UI will get an immediate response that an update was triggered, so it adds the "Agent is updating..." event to the chat and waits. When step 8 finishes, an `UpdateEstimateResponse` event is saved and the polling brings it into the UI, showing "Estimate updated." Meanwhile, the Project page updated the estimate view accordingly.

**Error Handling:** If something fails (e.g., the AI cannot parse the files or times out), the `task_jobs` would be marked "failed". The UI would then show an error state (the estimate view might show "Generation failed, please try again"). The chat, if that’s how it was triggered, would get an `UpdateEstimateResponse` with `success=false` and an error message, which could be displayed to the user (e.g., "Failed to update estimate: \[error]"). The user can then possibly retry.

**LangGraph and LangChain:** The code references `langchain_core` and `langgraph.graph.StateGraph`. It appears LangGraph might be built atop a framework (maybe a custom one named LangGraph or part of LangChain) that allows defining these nodes and edges. The details of how threads and runs are managed are abstracted away. As an engineer, you might not need to modify LangGraph’s core often, but you might update the BAML prompts or add new functions (for example, to refine how video is handled or to use a different model).

**Development Note:** Because the monorepo includes both apps, during development you run both the Next.js dev server and the LangGraph server concurrently. The README shows commands like `pnpm run dev:ui` and `pnpm run dev:langgraph`. They communicate via the API (likely [http://localhost\:PORT](mdc:http:/localhost:PORT), configured by `LANGGRAPH_API_URL`). The BAML codegen ensures any changes to prompts or types are reflected on both sides so you don’t have mismatches.

For onboarding, the main takeaway is: **LangGraph service handles AI calls using defined workflows**. The Next.js app never calls OpenAI API directly; it always goes through LangGraph. This provides a clear separation of concerns and easier maintenance of prompt logic in one place (the BAML files). If you need to adjust how the AI responds (say, tweak the prompt to improve output), you do it in BAML and regenerate clients, rather than scattering prompt text in the JS code.

**Relevant LangGraph Components:**

* `apps/langgraph/` – contains the Python code. Notably:

  * `src/file_processor/graph.py` – defines the workflow (nodes/edges) for processing files and generating estimates.
  * `src/file_processor/state.py` – likely defines the `State` dataclass that holds inputs (files, maybe ai\_estimate, etc.) and `Configuration` for model settings.
  * `baml_client_py/` – auto-generated client from BAML, including Pydantic models for our data classes and async functions to call the API.
  * Possibly a FastAPI app file (not explicitly seen, could be in `src/langgraph/` or similar) that sets up the routes `/threads` and `/runs`. This might use the `estimate_graph.compile()` result to actually execute runs when requested.

* `baml_src/` – BAML definitions. We have `file_processor.baml` for the main logic (estimate and media processing), `chat.baml` for chat logic, and likely some configuration in there as well. These are central to understanding how the AI is being instructed.
