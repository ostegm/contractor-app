###############################################################################
#
#  Welcome to Baml! To use this generated code, please run the following:
#
#  $ pip install baml-py
#
###############################################################################

# This file was generated by BAML: please do not edit it. Instead, edit the
# BAML files and re-generate this code.
#
# ruff: noqa: E501,F401
# flake8: noqa: E501,F401
# pylint: disable=unused-import,line-too-long
# fmt: off

file_map = {
    
    "chat.baml": "\nclass BamlChatThread {\n    events Event[]\n}\n\nclass Event {\n    type AllowedTypes\n    data (UserInput | AssisantMessage | UpdateEstimateRequest | UpdateEstimateResponse)\n}\n\n// Event Types\nenum AllowedTypes {\n    UserInput\n    AssisantMessage\n    UpdateEstimateRequest\n    UpdateEstimateResponse\n}\n\nclass UserInput {\n    message string @description(\"The message from the user\")\n}\n\nclass AssisantMessage {\n    message string @description(\"The message from the assistant\")\n}\n\n\nclass UpdateEstimateRequest {\n    changes_to_make string @description(\"Detailed description of changes to make to the estimate\")\n}\n\nclass UpdateEstimateResponse {\n    success bool @description(\"Whether the update was successful\")\n    error_message string @description(\"The error message if the update was not successful\")\n}\n\n\nfunction DetermineNextStep(thread: BamlChatThread, current_estimate: ConstructionProjectData) -> Event {\n    client OpenaiFallback\n    prompt #\"\n    You're a construction estimator working with a client. Help the client with any questions including updating the estimate as needed.\n    Use the current estimate and the conversation history to determine the next step.\n\n    <current_estimate>\n    {{ current_estimate }}\n    </current_estimate>\n\n    <conversation_history>\n    {% for event in thread.events %}\n    <{{ event.type }}>\n    {{ event.data }}\n    </{{ event.type }}>\n    {% endfor %}\n    </conversation_history>\n    {{ ctx.output_format }}\n    \"#\n    \n}\n\n\ntest TestDetermineNextStep {\n    functions [DetermineNextStep]\n    args {\n        thread {\n            events [\n                {\n                    type \"UserInput\"\n                    data {\n                        message \"Hello, how are you?\"\n                    }\n                }\n            ]\n        }\n    }\n}",
    "clients.baml": "// https://docs.boundaryml.com/docs/snippets/clients/fallback\nclient<llm> OpenaiFallback {\n  provider fallback\n  options {\n    // This will try the clients in order until one succeeds\n    strategy [GPT41MiniWith2Retries, GPT41With2Retries]\n  }\n}\n\n// Learn more about clients at https://docs.boundaryml.com/docs/snippets/clients/overview\nclient<llm> LocalQwen3 {\n  provider \"openai-generic\"\n  options {\n    base_url \"http://localhost:1234/v1\"\n    model \"qwen3-8b-mlx\"\n  }\n}\n\n// Step 2b: Define BAML Clients\nclient<llm> GeminiProcessor {\n  provider google-ai\n  options {\n    // Pull model name from env or keep configurable if needed\n    model \"gemini-2.5-flash-preview-04-17\"\n    api_key env.GOOGLE_API_KEY\n\n  }\n}\n\nclient<llm> O4Mini {\n  provider openai\n  options {\n    model \"o4-mini-2025-04-16\"\n    api_key env.OPENAI_API_KEY\n  }\n}\n\nclient<llm> GPT41With2Retries {\n  provider openai\n  retry_policy Exponential\n  options {\n    model \"gpt-4.1\"\n    api_key env.OPENAI_API_KEY\n  }\n}\n\nclient<llm> GPT41MiniWith2Retries {\n  provider openai\n  retry_policy Exponential\n  options {\n    model \"gpt-4.1-mini\"\n    api_key env.OPENAI_API_KEY\n  }\n}\n\n// https://docs.boundaryml.com/docs/snippets/clients/retry\nretry_policy Exponential {\n  max_retries 2\n  // Strategy is optional\n  strategy {\n    type exponential_backoff\n    delay_ms 300\n    multiplier 1.5\n    max_delay_ms 10000\n  }\n}",
    "file_processor.baml": "// BAML file: apps/langgraph/src/file_processor/baml_src/file_processor.baml\n\nclass EstimateLineItem {\n  description string @description(\"Description of the work item or material\")\n  category string @description(\"Category of the item (e.g., Demo, Plumbing, Electrical, etc.)\")\n  subcategory string? @description(\"Subcategory for further classification\")\n  cost_range_min float @description(\"Minimum estimated cost in dollars\")\n  cost_range_max float @description(\"Maximum estimated cost in dollars\")\n  unit string? @description(\"Unit of measurement (e.g., hours, sq ft, linear ft)\")\n  quantity float? @description(\"Estimated quantity\")\n  assumptions string? @description(\"Key assumptions made for this line item\")\n  confidence_score string? @description(\"Confidence in the estimate based on the information provided: (High, Medium, Low)\")\n  notes string? @description(\"Additional notes or details\")\n}\n\nclass ConstructionProjectData {\n  project_description string @description(\"Brief summary of the project scope\")\n  estimated_total_min float? @description(\"Minimum total estimated cost\")\n  estimated_total_max float? @description(\"Maximum total estimated cost\")\n  estimated_timeline_days int? @description(\"Estimated project duration in days\")\n  key_considerations string[] @description(\"List of key considerations for this project\")\n  confidence_level string @description(\"Overall confidence level in the estimate (High, Medium, Low)\")\n  estimate_items EstimateLineItem[] @description(\"Line items for the estimate\")\n  next_steps string[] @description(\"Prioritized next steps for the contractor\")\n  missing_information string[] @description(\"Information needed to improve estimate accuracy\")\n  key_risks string[] @description(\"List of key risks or potential complications\")\n}\n\nclass InputFile {\n  name string\n  type string // \"mime_type\"\n  description string? // Optional description\n  content string?     // Text content or transcription\n  download_url string? // URL to download the file content\n  image_data image? // Optional image data https://docs.boundaryml.com/ref/baml/types#image\n  audio_data audio? // Optional audio data https://docs.boundaryml.com/ref/baml/types#audio\n}\n\nfunction GenerateProjectEstimate(files: InputFile[], existing_estimate: ConstructionProjectData?, requested_changes: string?) -> ConstructionProjectData {\n  client OpenaiFallback\n  prompt #\"\n  You are an AI assistant specialized in analyzing construction project documents and media.\n  Your task is to synthesize information from various sources (text files, image descriptions, audio transcriptions)\n  and generate a structured cost estimate for a construction project.\n  Focus on extracting key details relevant to scope, materials, potential issues, or requirements mentioned in the files.\n\n  <UserProvidedFiles>\n  {% for file in files %}\n  <file name={{ file.name }} type={{ file.type }} description={{ file.description }}>\n  {% if file.image_data %}\n  {{ file.image_data }}\n  {% else %}\n  {{ file.content }}\n  {% endif %}\n  </file>\n  {% endfor %}\n  </UserProvidedFiles>\n\n  {% if existing_estimate %}\n  <ExistingEstimate>\n  {{ existing_estimate }}\n  </ExistingEstimate>\n  {% endif %}\n\n  {% if requested_changes %}\n  The user has requested the following changes to the existing estimate:\n  <RequestedChanges>\n  {{ requested_changes }}\n  </RequestedChanges>\n  {% endif %}\n\n  Based *solely* on the provided information, generate a detailed estimate including a project description, total estimated cost, and a list of line items with their individual costs.\n  Output the estimate as a JSON object conforming to the specified schema.\n\n  {{ ctx.output_format }}\n  \"#\n}\n\ntest TestGenerateProjectEstimate {\n  functions [GenerateProjectEstimate]\n  args {\n    files [\n      {name \"measurements.txt\", type \"text\", description \"measurements of the bathroom\", content \"10x10\"},\n      {name \"existing_bathroom.jpg\", type \"image\", description \"existing bathroom\", image_data {\n        file \"../apps/langgraph/tests/testdata/dated-bathroom.png\"\n      }},\n    ]\n  }\n}\n\nfunction ProcessAudio(audio: InputFile) -> string {\n  client GeminiProcessor\n  prompt #\"\n    You're an expert construction estimator. You're given an audio file of a construction project.\n    Your task is to review the audio and provide a transcription of the audio.\n\n    <Audio>\n    {{ audio.audio_data }}\n    </Audio>\n\n    Output the transcription as a string.\n\n  \"#\n}\n\ntest TestProcessAudio {\n  functions [ProcessAudio]\n  args {\n    audio {\n      name \"walkthrough_notes.m4a\", type \"audio\", description \"walkthrough notes\", audio_data {\n        file \"../apps/web/supabase/storage_seed_files/contractor-app-dev/project2/walkthrough_notes.m4a\"\n      }\n    }\n  }\n}\n\nclass KeyFrame {\n  filename string @description(\"Filename of the frame\")\n  timestamp_s float @description(\"Timestamp of the frame in seconds from the start of the video\")\n  description string @description(\"Detailed description of the frame and its relevance to the project\")\n}\n\nclass VideoAnalysis {\n  detailed_description string @description(\"A detailed summary of the video content, quoting user narration if present, and referencing key frame filenames to illustrate points. This description should be comprehensive enough for a downstream estimator to understand the video's content without watching it.\")\n  key_frames KeyFrame[] @description(\"A list of 10-20 key frames that capture the most informative visual details from the video relevant to project estimation.\")\n}\n\nfunction AnalyzeVideo(video_reference: string) -> VideoAnalysis {\n  client GeminiProcessor\n  prompt #\"\n  You are a construction-estimation assistant. You are given a reference to a video walkthrough of a project site.\n  Your task is to analyze the video thoroughly and extract key visual information and spoken narration to aid in project estimation.\n\n  The video reference is: {{ video_reference }}\n\n  Follow these instructions carefully:\n  1. Identify and select between 10 to 20 key frames from the video that are most informative for understanding the project scope, existing conditions, materials, and potential challenges.\n     For each key frame, provide:\n     - A descriptive `filename` (e.g., \"frame_01.png\", \"frame_02.png\", ...).\n     - The `timestamp_s` in seconds from the beginning of the video where the frame occurs.\n     - A detailed `description` of what is visually depicted in the frame and its significance to the project. Note any specific items, conditions, or measurements visible.\n\n  2. Create a `detailed_description` of the entire video. This should be a narrative summary that:\n     - Integrates information from both the visual content and any spoken narration in the video.\n     - Quotes important phrases from the user's narration, if any.\n     - References the `filename` of the key frames you selected to illustrate specific points in your summary (e.g., \"As seen in frame_03.png, the northern wall shows signs of water damage...\").\n     - Is comprehensive enough that a downstream estimator, who will *not* see the video, can understand the project's key aspects.\n\n  3. Return your analysis as a JSON object that strictly adheres to the `VideoAnalysis` schema, containing `detailed_description` and the list of `key_frames`.\n\n  {{ ctx.output_format }}\n  \"#\n}\n",
    "generators.baml": "generator ts_client {\n  output_type        \"typescript/react\"\n  output_dir         \"../apps/web/baml_client\"\n  version            \"0.86.1\"\n  default_client_mode async\n}\n\ngenerator py_client {\n  output_type        \"python/pydantic\"\n  output_dir         \"../apps/langgraph/src/file_processor\"\n  version            \"0.86.1\"\n  default_client_mode async\n}",
}

def get_baml_files():
    return file_map